import pyspark
from os import listdir
from os.path import isfile, join
import boto3
import pandas as pd
from sagemaker import get_execution_role
from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType
import pyspark.sql.functions as sf
conf = pyspark.SparkConf().setAppName('odl').setMaster('local')
sc = pyspark.SparkContext(conf=conf)
sqlc = pyspark.sql.SQLContext(sc)
sc
sqlc

role = get_execution_role()
bucket='odl-spark19spds6003-001'
data_key = 'code/admissions_data.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
#pd.read_csv(data_location)

df = sqlc.createDataFrame(pd.read_csv(data_location))
df
parquetPath = '/home/ec2-user/SageMaker/code/parquet_tmp'
df.write.parquet(parquetPath)
# prep list of files to transfer
files = [f for f in listdir(parquetPath) if isfile(join(parquetPath, f))]

s3 = boto3.resource('s3')
for f in files:
    #print('copying {} to {}'.format(parquetPath+'/'+f,"sample_data/"+f))
    s3.Bucket(bucket).upload_file(parquetPath+'/'+f, "code/"+f)
    # write to spark df
df = sqlc.read.parquet(parquetPath)
df

display(df)
df.printSchema()
# Dependent variable is Admission (Chance of admit)
print("Pearson's r(GRE,TOEFL) = {}".format(df.corr("GRE", "TOEFL")))
print("Pearson's r(SOP,LOR) = {}".format(df.corr("SOP", "LOR")))
print("Pearson's r(CGPA,Admission) = {}".format(df.corr("CGPA", "Admission")))
print("Pearson's r(GRE,Admission) = {}".format(df.corr("GRE", "Admission")))
print("Pearson's r(SOP,Admission) = {}".format(df.corr("SOP", "Admission")))

df = df.select("GRE","TOEFL","SOP","LOR","CGPA","Admission")
# create train/test sets
seed = 42
(testDF, trainingDF) = df.randomSplit((0.40, 0.60), seed=seed)
print ('training set N = {}, test set N = {}'.format(trainingDF.count(),testDF.count()))
from pyspark.ml.linalg import Vectors, VectorUDT # nb: bad form, done for pedagogy
#vectorization
# make a user defined function (udf)
sqlc.registerFunction("oneElementVec", lambda d: Vectors.dense([d]), returnType=VectorUDT())

# vectorize the data frames
trainingDF = trainingDF.selectExpr("Admission", "oneElementVec(GRE) as GRE","oneElementVec(TOEFL) as TOEFL" ,
                                   "oneElementVec(SOP) as SOP","oneElementVec(LOR) as LOR","oneElementVec(CGPA) as CGPA"                                  )
testDF = testDF.selectExpr("Admission", "oneElementVec(GRE) as GRE","oneElementVec(TOEFL) as TOEFL" ,
                                   "oneElementVec(SOP) as SOP","oneElementVec(LOR) as LOR","oneElementVec(CGPA) as CGPA")
print(testDF.orderBy(testDF.Admission.desc()).limit(5))

# rename to make ML engine happy
trainingDF = trainingDF.withColumnRenamed("Admission", "label")
testDF = testDF.withColumnRenamed("Admission", "label")

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = ['GRE', 'TOEFL' ,'SOP', 'LOR', 'CGPA'], outputCol = 'features')
trainingDF = vectorAssembler.transform(trainingDF)
trainingDF = trainingDF.select(['features', 'label'])
#trainingDF.show(3)

testDF = vectorAssembler.transform(testDF)
testDF = testDF.select(['features', 'label'])
testDF.show(3)

from pyspark.ml.regression import LinearRegression, LinearRegressionModel

lr = LinearRegression()
lrModel = lr.fit(trainingDF)

trainingDF.take(5)

# We are now going to transform our test set to get predictions. 
# It will append a prediction column to testDF in the new dataframe predictionsAndLabelsDF.
predictionsAndLabelsDF = lrModel.transform(testDF)
print(predictionsAndLabelsDF.orderBy(predictionsAndLabelsDF.label.desc()))

# calculate residuals
predictionsAndLabelsDF = predictionsAndLabelsDF.withColumn('residuals', predictionsAndLabelsDF.prediction-predictionsAndLabelsDF.label)
#predictionsAndLabelsDF.show()
pdf = predictionsAndLabelsDF.toPandas()
pdf.head()


import matplotlib.pyplot as plt

x = pdf['prediction']
y = pdf['residuals']

fig = plt.figure(figsize=(8,5))
plt.scatter(x, y, alpha=0.5)
# format plots
fig.suptitle('Residual vs Fitted Plot', fontsize=12)
plt.xlabel('Fitted Values', fontsize=8)
plt.ylabel('Residuals', fontsize=8)

plt.show()


from pyspark.ml.evaluation import RegressionEvaluator
eval = RegressionEvaluator()
print(eval.explainParams())



type(eval)




print("RMSE: %g" % eval.setMetricName("rmse").evaluate(predictionsAndLabelsDF))
print("R-Square: %g" %  eval.setMetricName("r2").evaluate(predictionsAndLabelsDF))
print("Mean Square Error: %g" % eval.setMetricName("mse").evaluate(predictionsAndLabelsDF))
